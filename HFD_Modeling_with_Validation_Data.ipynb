{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d435350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import mean, std, cov, sqrt, log, sum\n",
    "from numpy.random import seed\n",
    "\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "from scipy.stats import pearsonr,spearmanr, boxcox\n",
    "from scipy import stats\n",
    "\n",
    "from random import sample\n",
    "\n",
    "from apyori import apriori\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from matplotlib_venn import venn2\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import matplotlib.style as style\n",
    "\n",
    "from pmdarima.arima.utils import ndiffs\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import pmdarima as pm\n",
    "\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scripts as src\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# style.use('fivethirtyeight')\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "rcParams['figure.figsize'] = (6, 3)\n",
    "\n",
    "rand_state=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ae9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import catboost as ctb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, auc\n",
    "\n",
    "import tensorflow.keras\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321cbf5d",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df045ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import data\n",
    "raw_beneficiary=pd.read_csv('data/Train_Beneficiarydata-1542865627584.csv')\n",
    "raw_inpatient=pd.read_csv('data/Train_Inpatientdata-1542865627584.csv')\n",
    "raw_outpatient=pd.read_csv('data/Train_Outpatientdata-1542865627584.csv')\n",
    "raw_train=pd.read_csv('data/Train-1542865627584.csv')\n",
    "\n",
    "## merge data\n",
    "inpatient_beneficiary=pd.merge(raw_inpatient,raw_beneficiary,on='BeneID')\n",
    "outpatient_beneficiary=pd.merge(raw_outpatient,raw_beneficiary,on='BeneID')\n",
    "\n",
    "outpatient_beneficiary['Category']='Outpatient'\n",
    "inpatient_beneficiary['Category']='Inpatient'\n",
    "\n",
    "raw=pd.concat([outpatient_beneficiary, inpatient_beneficiary], axis = 0)\n",
    "raw=pd.merge(raw,raw_train,on='Provider', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = raw['PotentialFraud']\n",
    "X = raw.drop(['PotentialFraud'], axis=1) # becareful inplace= False\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_state)\n",
    "df_train=pd.concat([X_train, y_train], axis=1)\n",
    "df_test=pd.concat([X_test, y_test], axis=1)\n",
    "print('train data size:',df_train.shape,\n",
    "     '\\ntrain data size:',df_test.shape,\n",
    "     '\\ntotal:',len(df_train)+len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b5c57",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df_raw):\n",
    "    ## Ages\n",
    "    df_raw[['ClaimStartDt', 'ClaimEndDt', 'DischargeDt', 'AdmissionDt', 'DOB', 'DOD']]= \\\n",
    "            df_raw[['ClaimStartDt', 'ClaimEndDt', 'DischargeDt', 'AdmissionDt', 'DOB', 'DOD']].apply(pd.to_datetime, format='%Y-%m-%d')\n",
    "\n",
    "    df_raw['IsDead']=np.where(df_raw['DOD'].isna(), 'No', 'Yes')\n",
    "\n",
    "    df_raw['AdmissionDt']=df_raw['AdmissionDt'].fillna(df_raw['ClaimStartDt'])\n",
    "\n",
    "    df_raw['Age']=round((pd.to_datetime(df_raw['AdmissionDt'])-pd.to_datetime(df_raw['DOB'])).dt.total_seconds() / (24 * 60 * 60 * 365),1)\n",
    "\n",
    "    df_raw['Age_group'] = pd.cut(df_raw['Age'],\n",
    "                         bins=[0,25, 35, 45,55,65,75,85,90,100,120],\n",
    "                         labels=['<25','25-35','35-45','45-55','55-65','65-75','75-85','85-90','95-100','>100'],\n",
    "                         right=False)\n",
    "\n",
    "    df=df_raw[['Provider','Age']].groupby(['Provider'])['Age'].mean().reset_index().rename(columns={'Age':'AverageAge'})\n",
    "    df_raw=pd.merge(df_raw, df, on='Provider', how='left')\n",
    "\n",
    "    ## RenalDiseaseIndicator\n",
    "    df_raw['RenalDiseaseIndicator']=np.where((df_raw['RenalDiseaseIndicator'])=='0', 0, 1)\n",
    "\n",
    "    ## Gender \n",
    "    df_raw = df_raw.replace({'Gender': 2}, 'Female')\n",
    "    df_raw = df_raw.replace({'Gender': 1}, 'Male')\n",
    "\n",
    "    ## Chronic Features\n",
    "    df_raw = df_raw.replace({'ChronicCond_Alzheimer': 2, 'ChronicCond_Heartfailure': 2, 'ChronicCond_KidneyDisease': 2,\n",
    "                       'ChronicCond_Cancer': 2, 'ChronicCond_ObstrPulmonary': 2, 'ChronicCond_Depression': 2, \n",
    "                       'ChronicCond_Diabetes': 2, 'ChronicCond_IschemicHeart': 2, 'ChronicCond_Osteoporasis': 2, \n",
    "                       'ChronicCond_rheumatoidarthritis': 2, 'ChronicCond_stroke': 2 }, 0)\n",
    "\n",
    "    chronic_list=['ChronicCond_Alzheimer', 'ChronicCond_Heartfailure', 'ChronicCond_KidneyDisease', \n",
    "                  'ChronicCond_Cancer', 'ChronicCond_ObstrPulmonary', 'ChronicCond_Depression', \n",
    "                  'ChronicCond_Diabetes', 'ChronicCond_IschemicHeart',\n",
    "                  'ChronicCond_Osteoporasis', 'ChronicCond_rheumatoidarthritis', 'ChronicCond_stroke','RenalDiseaseIndicator']\n",
    "\n",
    "    # add mean of chronic condition numbers in claims by providers\n",
    "    df_raw['Chronic_number']=df_raw[chronic_list].sum(axis=1)\n",
    "\n",
    "    df=df_raw[chronic_list+['Provider','Chronic_number']].groupby(['Provider']).mean().reset_index()\n",
    "\n",
    "    df_raw.drop(chronic_list+['Chronic_number'], axis=1, inplace=True)\n",
    "    df_raw=pd.merge(df_raw, df, on='Provider', how='left')\n",
    "\n",
    "    ## Treatment Duration\n",
    "    df_raw['ClaimStartWeek']=df_raw['ClaimStartDt'].dt.week\n",
    "    df_raw['ClaimStartYear']=df_raw['ClaimStartDt'].dt.year\n",
    "\n",
    "    df_raw['DischargeDt']=df_raw['DischargeDt'].fillna(df_raw['AdmissionDt'])\n",
    "\n",
    "    df_raw['TreatmentDuration']=(df_raw['DischargeDt']-df_raw['AdmissionDt']).astype('timedelta64[D]').astype('int', errors='ignore')\n",
    "    \n",
    "    ## Diagnosis and Procedure codes\n",
    "    # fill in missing values\n",
    "    df_raw.dropna(how='all', inplace=True)\n",
    "\n",
    "    codelist=['ClmDiagnosisCode_1', 'ClmDiagnosisCode_2',\n",
    "           'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5',\n",
    "           'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8',\n",
    "           'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10', 'ClmProcedureCode_1',\n",
    "           'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4',\n",
    "           'ClmProcedureCode_5', 'ClmProcedureCode_6','ClmAdmitDiagnosisCode','DiagnosisGroupCode']\n",
    "\n",
    "    amountlist=['DeductibleAmtPaid']\n",
    "\n",
    "    for col in codelist:\n",
    "        df_raw[col]=df_raw[col].fillna('0')\n",
    "\n",
    "    for col in amountlist:\n",
    "        df_raw[col]=df_raw[col].fillna(0)  \n",
    "\n",
    "    def duplicate_claims(df, index, category, newcolname, threshold=0):\n",
    "        df_dup=df[[category,index]].drop_duplicates() \\\n",
    "                .groupby([index]).count().reset_index() \\\n",
    "                .rename( columns={category:newcolname}).sort_values(by=newcolname, ascending=False)\n",
    "\n",
    "        df_dup=pd.merge(df, df_dup, on=index, how='left')\n",
    "        df_dup[newcolname]=df_dup[newcolname].fillna(0)\n",
    "        if threshold!=0:        \n",
    "            counts_total=df_dup[[category]].nunique()[0]\n",
    "            df_dup['IsTop'+index+'By'+category]=np.where((df_dup[newcolname]/counts_total) \\\n",
    "                    >=threshold, 'Yes', 'No')\n",
    "        return df_dup\n",
    "\n",
    "    # Duplicate Codes\n",
    "    basic_info=['BeneID', 'ClaimStartDt', 'Provider','PotentialFraud']\n",
    "    df=df_raw[basic_info]\n",
    "    duplicated = df.duplicated(keep=False)\n",
    "    some_duplicates = df[duplicated].sort_values(by=df.columns.to_list())\n",
    "\n",
    "    some_duplicates=pd.merge(some_duplicates.drop_duplicates(),\n",
    "                             df_raw,\n",
    "                             on=['BeneID', 'ClaimStartDt', 'Provider', 'PotentialFraud'], \n",
    "                             how='left')\n",
    "\n",
    "\n",
    "    # Add DuplicateClaimCounts\n",
    "    some_duplicates=duplicate_claims(some_duplicates, 'Provider', 'ClaimID', 'DuplicateClaimCounts', 0)\n",
    "    df_raw=pd.merge(df_raw, some_duplicates[['Provider','DuplicateClaimCounts']].drop_duplicates(), on='Provider', how='left')\n",
    "    df_raw['DuplicateClaimCounts']=df_raw['DuplicateClaimCounts'].fillna(0)\n",
    "\n",
    "    def IsDuplicateCode(df, index, colcode, newcolname):\n",
    "        df_dup_codes=df.groupby([index, colcode]).count()['ClaimID']\n",
    "        df_dup_codes=df_dup_codes.sort_values(ascending=False).reset_index()\n",
    "        df_dup_codes=df_dup_codes.replace('0',np.nan)\n",
    "        df_dup_codes.dropna(how='any', inplace=True)\n",
    "        df_dup_codes.rename( columns={'ClaimID':newcolname}, inplace=True)\n",
    "        ProviderList=df_dup_codes[df_dup_codes[newcolname]>1][index].unique()\n",
    "        return ProviderList\n",
    "\n",
    "    # Add IsDuplicateClmAdmDiagCode\n",
    "    ProviderList=IsDuplicateCode(some_duplicates, 'Provider', 'ClmAdmitDiagnosisCode', 'DuplicateAdmitDiagCode')\n",
    "    df_raw['IsDuplicateClmAdmDiagCode']=np.where(df_raw['Provider'].isin(ProviderList),'Yes','No')\n",
    "\n",
    "    # Add IsDuplicateClmAdmDiagCode\n",
    "    ProviderList=IsDuplicateCode(some_duplicates, 'Provider', 'DiagnosisGroupCode', 'DuplicateDiagnosisGroupCode')\n",
    "    df_raw['IsDuplicateDiagGrpCode']=np.where(df_raw['Provider'].isin(ProviderList),'Yes','No')\n",
    "\n",
    "    # Most frequently used diagnosis codes codes\n",
    "    def most_codes(df):\n",
    "        df_codes=pd.DataFrame()\n",
    "        for col in df.columns:\n",
    "            df_codes[col]=df.groupby([col]).count().iloc[:,1].sort_values(ascending=False)  \n",
    "        if df_codes.index.isin(['nan']).any():\n",
    "            df_codes.drop(['nan'], inplace=True)   \n",
    "        if df_codes.index.isin(['0']).any():\n",
    "            df_codes.drop(['0'], inplace=True)   \n",
    "        df_codes=df_codes.sum(axis=1).sort_values(ascending=False)\n",
    "        return df_codes\n",
    "\n",
    "    Diagnosiscodelist=['ClmDiagnosisCode_1', 'ClmDiagnosisCode_2',\n",
    "           'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5',\n",
    "           'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8',\n",
    "           'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10']\n",
    "    Procedurecodelist=[\n",
    "           'ClmProcedureCode_1', 'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4',\n",
    "           'ClmProcedureCode_5', 'ClmProcedureCode_6']\n",
    "\n",
    "    codelist=Diagnosiscodelist+Procedurecodelist\n",
    "\n",
    "    df_diagnosis=df_raw[Diagnosiscodelist]\n",
    "    df_procedure=df_raw[Procedurecodelist]\n",
    "    diagnosiscodelist=most_codes(df_diagnosis).head(30).index.tolist()\n",
    "    procedurecodelist=most_codes(df_procedure).head(15).index.tolist()\n",
    "\n",
    "    df=pd.DataFrame()\n",
    "    for c in diagnosiscodelist:\n",
    "        l=df_diagnosis.isin([c]).sum(axis=1)\n",
    "        df['Diag_'+c]=l\n",
    "    df_raw=pd.concat([df_raw, df], axis=1)\n",
    "\n",
    "    df1=df_raw[Diagnosiscodelist+['Provider']].groupby(['Provider']).mean().reset_index()\n",
    "\n",
    "    df=pd.DataFrame()\n",
    "    for c in procedurecodelist:\n",
    "        l=df_diagnosis.isin([c]).sum(axis=1)\n",
    "        df['Proc_'+str(c)]=l\n",
    "    df_raw=pd.concat([df_raw, df], axis=1)\n",
    "    df2=df_raw[Procedurecodelist+['Provider']].groupby(['Provider']).mean().reset_index()\n",
    "\n",
    "    df_raw.drop(codelist, axis=1, inplace=True)\n",
    "    df_raw=pd.merge(df_raw, df1, on='Provider', how='left')\n",
    "    df_raw=pd.merge(df_raw, df2, on='Provider', how='left')\n",
    "\n",
    "    ## Summary\n",
    "    df_raw['AnnualReimbursementAmt']=df_raw['IPAnnualReimbursementAmt'] + df_raw['OPAnnualReimbursementAmt']\n",
    "    df_raw['AnnualDeductibleAmt']=df_raw['IPAnnualDeductibleAmt'] + df_raw['OPAnnualDeductibleAmt']\n",
    "    df_raw['TotalPayment']=df_raw['DeductibleAmtPaid']+df_raw['InscClaimAmtReimbursed']\n",
    "\n",
    "    ## Provider Features\n",
    "    physicians=pd.melt(df_raw[['BeneID', 'ClaimID', 'Provider', 'Category',\n",
    "           'AttendingPhysician', 'OperatingPhysician', 'OtherPhysician', 'PotentialFraud']], \n",
    "            id_vars=['BeneID','ClaimID', 'Provider','PotentialFraud','Category'], \n",
    "            value_vars=df_raw[['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']], \n",
    "            var_name='Type', \n",
    "            value_name='Physician').sort_values(by=['ClaimID','Type']).dropna(subset=['Physician'])\n",
    "\n",
    "    # Add Patient Counts For Providers\n",
    "    # the providers with patient counts over 900 or 0.006 are mostly suspective of frauds\n",
    "    # So add IsTopProviderByPatients (0.005 used to hold more erros)\n",
    "    df_raw=duplicate_claims(df_raw, 'Provider', 'BeneID', 'PatientCountsByProviders', 0.005)\n",
    "\n",
    "    # Add Physician Counts For Providers\n",
    "    df=duplicate_claims(physicians, 'Provider', 'Physician', 'PhysicianCountsByProviders', 0)\n",
    "    df=df[['Provider','PhysicianCountsByProviders']].drop_duplicates()\n",
    "    df_raw=pd.merge(df_raw, df, on='Provider', how='left')\n",
    "\n",
    "    # Add Claim Counts For Providers\n",
    "    # the providers with claim counts over 1300 or 0.002 of all claims are mostly suspective of frauds\n",
    "    # so Add IsTopProviderByClaimCounts (0.001 used to hold more erros)\n",
    "    df_raw=duplicate_claims(df_raw, 'Provider', 'ClaimID', 'ClaimCountsByProviders', 0.001)\n",
    "    \n",
    "    ## Time Series Terms\n",
    "    df_raw['ClaimDayOfWeek']=df_raw['ClaimStartDt'].dt.weekday\n",
    "\n",
    "    df_raw['Week_start']=df_raw['ClaimStartDt'] - df_raw['ClaimDayOfWeek'] * np.timedelta64(1, 'D')\n",
    "    df_raw['Week_end']= df_raw['Week_start'] + timedelta(days=6)\n",
    "\n",
    "    df_raw=pd.merge(df_raw, df_raw.groupby(['ClaimStartYear','ClaimStartWeek','Category'])['ClaimID'].count(), \n",
    "                   on=['ClaimStartYear','ClaimStartWeek','Category'], how='left')\n",
    "\n",
    "    df_raw= df_raw.rename( columns={'ClaimID_x':'ClaimID',\n",
    "                             'ClaimID_y':'WeeklyClaimCountsByCategory'})\n",
    "\n",
    "    df_raw['LogWeeklyCounts']=np.log(df_raw['WeeklyClaimCountsByCategory'])\n",
    "\n",
    "    def ts_terms(df, category):   \n",
    "\n",
    "        ts_category=df[df['Category']==category]\n",
    "\n",
    "        ts_features_stat=ts_category[['LogWeeklyCounts','ClaimStartYear','ClaimStartWeek']].drop_duplicates() \\\n",
    "            .sort_values(['ClaimStartYear','ClaimStartWeek'])\n",
    "\n",
    "        ts_features_stat=ts_features_stat.set_index(['ClaimStartYear','ClaimStartWeek'])\n",
    "\n",
    "        ## Add AutoRegressive Terms\n",
    "        ts_features_stat['lag_1']=ts_features_stat['LogWeeklyCounts'].shift(1)\n",
    "        ts_features_stat['lag_2']=ts_features_stat['LogWeeklyCounts'].shift(2)\n",
    "\n",
    "        ## Add Moving Average Terms\n",
    "        ts_features_stat['rolling_mean_1'] = ts_features_stat['LogWeeklyCounts'].rolling(window=1).mean()\n",
    "        ts_features_stat['rolling_mean_2'] = ts_features_stat['LogWeeklyCounts'].rolling(window=2).mean()\n",
    "\n",
    "        ## Add Rolling term for 12 weeks\n",
    "        ts_features_stat['rolling_mean_12'] = ts_features_stat['LogWeeklyCounts'].rolling(window=12).mean()\n",
    "\n",
    "        ## Add differencing Terms\n",
    "        ts_features_stat['diff_1'] = ts_features_stat['LogWeeklyCounts'].diff()\n",
    "        ts_features_stat['diff_2'] = ts_features_stat['LogWeeklyCounts'].diff().diff()\n",
    "\n",
    "        ts_features_stat['Category']=category\n",
    "        ts_features_stat=ts_features_stat.reset_index()\n",
    "        ts_features_stat.fillna(ts_features_stat.mean(), inplace=True)\n",
    "\n",
    "        return ts_features_stat\n",
    "\n",
    "    ts_features=df_raw[['ClaimStartDt','Category','LogWeeklyCounts','ClaimStartYear','ClaimStartWeek']]\n",
    "\n",
    "    # Add ts features for outpatient\n",
    "    ts_features_outpatient=ts_terms(ts_features, 'Outpatient')\n",
    "    # Add ts features for inpatient\n",
    "    ts_features_inpatient=ts_terms(ts_features, 'Inpatient')\n",
    "\n",
    "    ts_features_allpatients=pd.concat([ts_features_inpatient,ts_features_outpatient])\n",
    "    ts_features=ts_features[['ClaimStartDt','Category','ClaimStartYear','ClaimStartWeek']]\n",
    "    ts_features=pd.merge(ts_features, ts_features_allpatients, on=['ClaimStartYear', 'ClaimStartWeek', 'Category'], how='left')\n",
    "    df_raw[['lag_1', 'lag_2', \n",
    "          'rolling_mean_1', 'rolling_mean_2', 'rolling_mean_12', \n",
    "          'diff_1', 'diff_2']]=ts_features[['lag_1', 'lag_2', \n",
    "          'rolling_mean_1', 'rolling_mean_2', 'rolling_mean_12', \n",
    "          'diff_1', 'diff_2']]\n",
    "    \n",
    "    ## calculate mean counts of each state, race, gender, age_group and county of claims for each provider\n",
    "    def pivot_category(df, index, category):\n",
    "        index_list=df[index].sort_values().unique().tolist()\n",
    "        category_list=df[category].unique().tolist()\n",
    "\n",
    "        df_counts=df[[index,category,'ClaimID']]\n",
    "        df_counts=df_counts.groupby([index,category]).count().reset_index()\n",
    "\n",
    "        df_out = pd.DataFrame([(i, s) for i in index_list for s in category_list], columns=[index,category])\n",
    "        df_out=pd.merge(df_out,df_counts,on=[index,category],how='left')\n",
    "        df_out.fillna(0,inplace=True) \n",
    "        df_out=df_out.pivot(index=index, columns=category, values='ClaimID'). \\\n",
    "                reset_index().add_prefix(category+'_').rename(columns={category+'_'+index:index})\n",
    "        df_out=pd.merge(df, df_out, on=index, how='left' )\n",
    "        return df_out\n",
    "\n",
    "    df_raw=pivot_category(df_raw,'Provider','State')    \n",
    "    df_raw=pivot_category(df_raw,'Provider','Race')    \n",
    "    df_raw=pivot_category(df_raw,'Provider','Gender')    \n",
    "    df_raw=pivot_category(df_raw,'Provider','Age_group')    \n",
    "    df_raw=pivot_category(df_raw,'Provider','County')    \n",
    "    df_raw=pivot_category(df_raw,'Provider','Category')\n",
    "    df_raw=pivot_category(df_raw,'Provider','IsDead')\n",
    "    \n",
    "    ## drop useless columns\n",
    "    droplist1=['ClaimStartDt', 'ClaimEndDt', 'DOB', 'DOD',\n",
    "     'Race', 'State', 'County', 'Category', 'AdmissionDt', 'IsDead',\n",
    "     'DischargeDt', 'Age', 'Age_group', 'ClaimStartWeek', 'ClaimStartYear',\n",
    "     'ClaimDayOfWeek', 'Week_start', 'Week_end', 'WeeklyClaimCountsByCategory']\n",
    "\n",
    "    droplist2=['BeneID', 'Provider', 'ClaimID', \n",
    "               'AttendingPhysician', 'OperatingPhysician', 'OtherPhysician', \n",
    "               'ClmAdmitDiagnosisCode', 'Gender', 'DiagnosisGroupCode']\n",
    "    df_raw.drop(droplist1+droplist2,axis=1,inplace=True)\n",
    "    \n",
    "    df_raw.fillna(0, inplace=True)\n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=preprocessing(df_train)\n",
    "test=preprocessing(df_test)\n",
    "\n",
    "# make train and test the same columns\n",
    "strain=set(train.columns)\n",
    "stest=set(test.columns)\n",
    "l_train=list(strain.difference(stest))\n",
    "l_test=list(stest.difference(strain))\n",
    "test.drop(columns =l_test, inplace=True)\n",
    "test[l_train]=0\n",
    "\n",
    "train.to_csv('data/train_after_processing.csv')\n",
    "test.to_csv('data/test_after_processing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=pd.read_csv('data/train_after_processing.csv')\n",
    "# test=pd.read_csv('data/test_after_processing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## usful functions\n",
    "def numeric_columns(df):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64','datetime64[ns]']\n",
    "    df_numeric = df.select_dtypes(include=numerics)\n",
    "    return df_numeric.columns\n",
    "\n",
    "def object_columns(df):\n",
    "    objects = ['object']\n",
    "    df_object = df.select_dtypes(include=objects)\n",
    "    return df_object.columns\n",
    "\n",
    "def cat_features(df, ls):\n",
    "    for l in ls:\n",
    "        df[l]=df[l].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd12e9b",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:powderblue;\">Modelling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33613374",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train['PotentialFraud'],train['PotentialFraud'], normalize='all')*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dea906",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test['PotentialFraud'],test['PotentialFraud'], normalize='all')*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b96b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "object_columns(train).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c4a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns=['PotentialFraud',\n",
    "     'IsDuplicateClmAdmDiagCode',\n",
    "     'IsDuplicateDiagGrpCode',\n",
    "     'IsTopProviderByBeneID',\n",
    "     'IsTopProviderByClaimID']\n",
    "train_dummies = pd.get_dummies(train, columns=category_columns, sparse=True, drop_first=True)\n",
    "train_yd = train_dummies['PotentialFraud_Yes']\n",
    "train_Xd = train_dummies.drop('PotentialFraud_Yes', axis=1) # becareful inplace= False\n",
    "\n",
    "sc = StandardScaler(with_mean=False)\n",
    "train_Xd_sc = sc.fit_transform(train_Xd)\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "train[category_columns]= enc.fit_transform(train[category_columns])\n",
    "\n",
    "train_y_ord = train['PotentialFraud']\n",
    "train_X_ord = train.drop(['PotentialFraud'], axis=1) # becareful inplace= False\n",
    "\n",
    "train_X_ord=train_X_ord.fillna(0)\n",
    "train_Xd=train_Xd.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7593eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dummies = pd.get_dummies(test, columns=category_columns, sparse=True, drop_first=True)\n",
    "test_yd = test_dummies['PotentialFraud_Yes']\n",
    "test_Xd = test_dummies.drop('PotentialFraud_Yes', axis=1) # becareful inplace= False\n",
    "\n",
    "sc = StandardScaler(with_mean=False)\n",
    "test_Xd_sc = sc.fit_transform(test_Xd)\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "test[category_columns]= enc.fit_transform(test[category_columns])\n",
    "\n",
    "test_y_ord = test['PotentialFraud']\n",
    "test_X_ord = test.drop(['PotentialFraud'], axis=1) # becareful inplace= False\n",
    "\n",
    "test_X_ord=test_X_ord.fillna(0)\n",
    "test_Xd=test_Xd.fillna(0)\n",
    "\n",
    "# train_Xd[train_Xd.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0966129",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_X_ord shape:', train_X_ord.shape)\n",
    "print('test_X_ord shape:', test_X_ord.shape)\n",
    "\n",
    "print('train_Xd_sc shape:', train_Xd_sc.shape)\n",
    "print('test_Xd_sc shape:', test_Xd_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc83210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting RF classifier to the Training set\n",
    "RF_classifier = RandomForestClassifier(random_state=rand_state)\n",
    "RF_classifier.fit(train_X_ord, train_y_ord)\n",
    "# Predicting the Test set probabilities and classes\n",
    "y_hat_RF       = RF_classifier.predict(test_X_ord)\n",
    "y_hat_RF_probs = RF_classifier.predict_proba(test_X_ord)\n",
    "print('accuracy = {}'.format(accuracy_score(test_y_ord, y_hat_RF)))\n",
    "print('f1 = {}'.format(f1_score(test_y_ord, y_hat_RF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting AdaBoost classifier to the Training set\n",
    "AdB_classifier = AdaBoostClassifier(random_state=rand_state)\n",
    "AdB_classifier.fit(train_X_ord, train_y_ord)\n",
    "y_hat_AdB      = AdB_classifier.predict(test_X_ord)\n",
    "y_hat_AdB_probs = AdB_classifier.predict_proba(test_X_ord)\n",
    "print('accuracy = {}'.format(accuracy_score(test_y_ord, y_hat_AdB)))\n",
    "print('f1 = {}'.format(f1_score(test_y_ord, y_hat_AdB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Gradient Boosting classifier to the Training set\n",
    "GBM_classifier = GradientBoostingClassifier(random_state=rand_state, learning_rate=0.1, max_depth=12)\n",
    "GBM_classifier.fit(train_X_ord, train_y_ord)\n",
    "y_hat_GBM      = GBM_classifier.predict(test_X_ord)\n",
    "y_hat_GBM_probs = GBM_classifier.predict_proba(test_X_ord)\n",
    "print('accuracy = {}'.format(accuracy_score(test_y_ord, y_hat_GBM)))\n",
    "print('f1 = {}'.format(f1_score(test_y_ord, y_hat_GBM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting XGBoost classifier to the Training set\n",
    "XGB_classifier = XGBClassifier(random_state=rand_state, eta=0.1, max_depth=12)\n",
    "XGB_classifier.fit(train_X_ord, train_y_ord)\n",
    "\n",
    "y_hat_XGB      = XGB_classifier.predict(test_X_ord)\n",
    "y_hat_XGB_probs = XGB_classifier.predict_proba(test_X_ord)\n",
    "print('accuracy = {}'.format(accuracy_score(test_y_ord, y_hat_XGB)))\n",
    "print('f1 = {}'.format(f1_score(test_y_ord, y_hat_XGB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61477dd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CBC_classifier = ctb.CatBoostClassifier(depth=10, random_state=rand_state)\n",
    "CBC_classifier.fit(train_X_ord, train_y_ord)\n",
    "y_hat_CBC      = CBC_classifier.predict(test_X_ord)\n",
    "y_hat_CBC_probs = CBC_classifier.predict_proba(test_X_ord)\n",
    "print('accuracy = {}'.format(accuracy_score(test_y_ord, y_hat_CBC)))\n",
    "print('f1 = {}'.format(f1_score(test_y_ord, y_hat_CBC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961958bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_classifier = lgb.LGBMClassifier()\n",
    "LGB_classifier.fit(train_X_ord, train_y_ord)\n",
    "y_hat_LGB     = LGB_classifier.predict(test_X_ord)\n",
    "y_hat_LGB_probs = LGB_classifier.predict_proba(test_X_ord)\n",
    "print('accuracy = {}'.format(accuracy_score(test_y_ord, y_hat_LGB)))\n",
    "print('f1 = {}'.format(f1_score(test_y_ord, y_hat_LGB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d08a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic_classifier = LogisticRegression(random_state=rand_state)\n",
    "Logistic_classifier.fit(train_Xd_sc, train_yd)\n",
    "yd_hat_Logistic      = Logistic_classifier.predict(test_Xd_sc)\n",
    "yd_hat_Logistic_probs = Logistic_classifier.predict_proba(test_Xd_sc)\n",
    "print('accuracy = {}'.format(accuracy_score(test_yd, yd_hat_Logistic)))\n",
    "print('f1 = {}'.format(f1_score(test_yd, yd_hat_Logistic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_classifier = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", random_state=rand_state)\n",
    "Lasso_classifier.fit(train_Xd_sc, train_yd)\n",
    "yd_hat_Lasso      = Lasso_classifier.predict(test_Xd_sc)\n",
    "yd_hat_Lasso_probs = Lasso_classifier.predict_proba(test_Xd_sc)\n",
    "print('accuracy = {}'.format(accuracy_score(test_yd, yd_hat_Lasso)))\n",
    "print('f1 = {}'.format(f1_score(test_yd, yd_hat_Lasso)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge_classifier = LogisticRegression(penalty='l2', solver='liblinear', random_state=rand_state)\n",
    "Ridge_classifier.fit(train_Xd_sc, train_yd)\n",
    "yd_hat_Ridge      = Ridge_classifier.predict(test_Xd_sc)\n",
    "yd_hat_Ridge_probs = Ridge_classifier.predict_proba(test_Xd_sc) \n",
    "print('accuracy = {}'.format(accuracy_score(test_yd, yd_hat_Ridge)))\n",
    "print('f1 = {}'.format(f1_score(test_yd, yd_hat_Ridge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ee717",
   "metadata": {},
   "outputs": [],
   "source": [
    "ElasticNet_classifier = LogisticRegression(penalty='elasticnet', solver='saga',l1_ratio=0.5, random_state=rand_state)\n",
    "ElasticNet_classifier.fit(train_Xd_sc, train_yd)\n",
    "yd_hat_ElasticNet      = ElasticNet_classifier.predict(test_Xd_sc)\n",
    "yd_hat_ElasticNet_probs = ElasticNet_classifier.predict_proba(test_Xd_sc)\n",
    "print('accuracy = {}'.format(accuracy_score(test_yd, yd_hat_ElasticNet)))\n",
    "print('f1 = {}'.format(f1_score(test_yd, yd_hat_ElasticNet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "KNN_classifier.fit(train_Xd_sc, train_yd)\n",
    "yd_hat_KNN      = KNN_classifier.predict(test_Xd_sc)\n",
    "yd_hat_KNN_probs = KNN_classifier.predict_proba(test_Xd_sc)\n",
    "print('accuracy = {}'.format(accuracy_score(test_yd, yd_hat_KNN)))\n",
    "print('f1 = {}'.format(f1_score(test_yd, yd_hat_KNN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb07ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification neural network\n",
    "\n",
    "seed(rand_state)\n",
    "tensorflow.random.set_seed(rand_state)\n",
    "Neural_classifier = Sequential()\n",
    "Neural_classifier.add(Dense(100, input_dim=train_Xd_sc.shape[1], activation='relu',\n",
    "                kernel_initializer='random_normal'))\n",
    "Neural_classifier.add(Dense(50,activation='relu',kernel_initializer='random_normal'))\n",
    "Neural_classifier.add(Dense(25,activation='relu',kernel_initializer='random_normal'))\n",
    "Neural_classifier.add(Dense(1,activation='sigmoid',kernel_initializer='random_normal'))\n",
    "Neural_classifier.compile(loss='binary_crossentropy', \n",
    "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "              metrics =['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, \n",
    "    patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "Neural_classifier.fit(train_Xd_sc,train_yd,validation_data=(test_Xd_sc,test_yd),\n",
    "          callbacks=[monitor],verbose=2,epochs=30)\n",
    "\n",
    "yd_hat_Neural = np.round(Neural_classifier.predict(test_Xd_sc))\n",
    "yd_hat_Neural_probs = Neural_classifier.predict(test_Xd_sc)\n",
    "print('accuracy = {}'.format(accuracy_score(test_yd, yd_hat_Neural)))\n",
    "print('f1 = {}'.format(f1_score(test_yd, yd_hat_Neural)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff643258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blended_predict(train_or_test):\n",
    "#     if train_or_test=='train':\n",
    "#         return ((0.5 * CBC_classifier.predict(train_X_ord)) + \\\n",
    "#                 (0.3 * XGB_classifier.predict(train_X_ord)) + \\\n",
    "#                 (0.1 * RF_classifier.predict(train_X_ord)) + \\\n",
    "#                 (0.1 * GBM_classifier.predict(train_X_ord)) )\n",
    "#     else:\n",
    "#         return ((0.5 * CBC_classifier.predict(test_X_ord)) + \\\n",
    "#                 (0.3 * XGB_classifier.predict(test_X_ord)) + \\\n",
    "#                 (0.1 * RF_classifier.predict(test_X_ord)) + \\\n",
    "#                 (0.1 * GBM_classifier.predict(test_X_ord)) )\n",
    "    if train_or_test=='train':\n",
    "        return ((0.5 * AdB_classifier.predict(train_X_ord)) + \\\n",
    "                (0.3 * Logistic_classifier.predict(train_Xd_sc)) + \\\n",
    "                (0.1 * ElasticNet_classifier.predict(train_Xd_sc)) + \\\n",
    "                (0.1 * CBC_classifier.predict(train_X_ord)) )\n",
    "    else:\n",
    "        return ((0.5 * AdB_classifier.predict(test_X_ord)) + \\\n",
    "                (0.3 * Logistic_classifier.predict(test_Xd_sc)) + \\\n",
    "                (0.1 * ElasticNet_classifier.predict(test_Xd_sc)) + \\\n",
    "                (0.1 * CBC_classifier.predict(test_X_ord)) )\n",
    "\n",
    "def score_list(df, regr,  y_test=None, y_hat_test=None):\n",
    "    if regr=='Blended Model':\n",
    "        y_hat_blended_train=blended_predict('train')\n",
    "        y_hat_blended_test=blended_predict('test')\n",
    "        \n",
    "        accuracy=accuracy_score(y_test, np.round(y_hat_blended_test))\n",
    "        f1=f1_score(y_test, np.round(y_hat_blended_test))\n",
    "        precision=precision_score(y_test, np.round(y_hat_blended_test), average='binary')\n",
    "        recall=recall_score(y_test, np.round(y_hat_blended_test), average='binary')\n",
    "    else:\n",
    "        accuracy=accuracy_score(y_test, y_hat_test)\n",
    "        f1=f1_score(y_test, y_hat_test)\n",
    "        precision=precision_score(y_test, y_hat_test, average='binary')\n",
    "        recall=recall_score(y_test, y_hat_test, average='binary')    \n",
    "\n",
    "\n",
    "    df[regr]=[accuracy, f1, precision, recall]\n",
    "    return df \n",
    "\n",
    "df=pd.DataFrame()              \n",
    "    \n",
    "# df['Scores']=['Train Score', 'Test Score', 'Train MSE', 'Test MSE', 'Train RMSE', 'Test RMSE']\n",
    "df=score_list(df, 'Logistic Regression',  test_yd, yd_hat_Logistic)\n",
    "df=score_list(df, 'Lasso',  test_yd, yd_hat_Lasso)\n",
    "df=score_list(df, 'Ridge', test_yd, yd_hat_Ridge)\n",
    "df=score_list(df, 'ElasticNet', test_yd, yd_hat_ElasticNet)\n",
    "df=score_list(df, 'KNN', test_yd, yd_hat_KNN)\n",
    "df=score_list(df, 'Random Forest', test_y_ord, y_hat_RF)\n",
    "df=score_list(df, 'GBM', test_y_ord, y_hat_GBM)\n",
    "df=score_list(df, 'ADA', test_y_ord, y_hat_AdB)\n",
    "df=score_list(df, 'XGBoost', test_y_ord, y_hat_XGB)\n",
    "df=score_list(df, 'Cat Boost', test_y_ord, y_hat_CBC)\n",
    "df=score_list(df, 'LightGBM', test_y_ord, y_hat_LGB)\n",
    "df=score_list(df, 'Neural Network', test_yd, yd_hat_Neural)\n",
    "df=score_list(df, regr='Blended Model', y_test=test_y_ord)\n",
    "\n",
    "df=df.transpose()\n",
    "df.columns=['Accuracy', 'F1', 'Precision', 'Recall']\n",
    "df.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4443c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(X_train.columns)\n",
    "\n",
    "RF_importance  = RF_classifier.feature_importances_\n",
    "AdB_importance = AdB_classifier.feature_importances_\n",
    "GBM_importance = GBM_classifier.feature_importances_\n",
    "XGB_importance = XGB_classifier.feature_importances_\n",
    "CBC_importance = CBC_classifier.feature_importances_\n",
    "LGB_importance = LGB_classifier.feature_importances_\n",
    "Logistic_importance = Logistic_classifier.coef_\n",
    "Lasso_importance= Lasso_classifier.coef_\n",
    "Ridge_importance= Ridge_classifier.coef_\n",
    "ElasticNet_importance= ElasticNet_classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "\n",
    "fpr1, tpr1, thresholds = roc_curve(test_y_ord, y_hat_RF_probs[:, 1], pos_label=1)\n",
    "fpr2, tpr2, thresholds = roc_curve(test_y_ord, y_hat_AdB_probs[:, 1], pos_label=1)\n",
    "fpr3, tpr3, thresholds = roc_curve(test_y_ord, y_hat_GBM_probs[:, 1], pos_label=1)\n",
    "fpr4, tpr4, thresholds = roc_curve(test_y_ord, y_hat_XGB_probs[:, 1], pos_label=1)\n",
    "fpr5, tpr5, thresholds = roc_curve(test_y_ord, y_hat_CBC_probs[:, 1], pos_label=1)\n",
    "fpr6, tpr6, thresholds = roc_curve(test_yd, yd_hat_Logistic_probs[:, 1], pos_label=1)\n",
    "fpr7, tpr7, thresholds = roc_curve(test_yd, yd_hat_Lasso_probs[:, 1], pos_label=1)\n",
    "fpr8, tpr8, thresholds = roc_curve(test_yd, yd_hat_Ridge_probs[:, 1], pos_label=1)\n",
    "fpr9, tpr9, thresholds = roc_curve(test_yd, yd_hat_ElasticNet_probs[:, 1], pos_label=1)\n",
    "fpr10, tpr10, thresholds = roc_curve(test_yd, yd_hat_KNN_probs[:, 1], pos_label=1)\n",
    "fpr11, tpr11, thresholds = roc_curve(test_yd, yd_hat_Neural_probs, pos_label=1)\n",
    "fpr12, tpr12, thresholds = roc_curve(test_y_ord, y_hat_LGB_probs[:, 1], pos_label=1)\n",
    "\n",
    "roc_auc1 = auc(fpr1, tpr1)\n",
    "roc_auc2 = auc(fpr2, tpr2)\n",
    "roc_auc3 = auc(fpr3, tpr3)\n",
    "roc_auc4 = auc(fpr4, tpr4)\n",
    "roc_auc5 = auc(fpr5, tpr5)\n",
    "roc_auc6 = auc(fpr6, tpr6)\n",
    "roc_auc7 = auc(fpr7, tpr7)\n",
    "roc_auc8 = auc(fpr8, tpr8)\n",
    "roc_auc9 = auc(fpr9, tpr9)\n",
    "roc_auc10 = auc(fpr10, tpr10)\n",
    "roc_auc11 = auc(fpr11, tpr11)\n",
    "roc_auc12 = auc(fpr12, tpr12)\n",
    "\n",
    "# auc = round(metrics.roc_auc_score(y_test, y_pred), 4)\n",
    "\n",
    "plt.plot(fpr5, tpr5, label='CatBoost  (AUC = %0.2f)' % (roc_auc5))\n",
    "plt.plot(fpr1, tpr1, label='Random Forest (AUC = %0.2f)' % (roc_auc1))\n",
    "plt.plot(fpr12, tpr12, label='LightGBM  (AUC = %0.2f)' % (roc_auc12))\n",
    "plt.plot(fpr4, tpr4, label='XGBoost  (AUC = %0.2f)' % (roc_auc4))\n",
    "plt.plot(fpr2, tpr2, label='AdaBoost (AUC = %0.2f)' % (roc_auc2))\n",
    "plt.plot(fpr3, tpr3, label='Gradient Boosting (AUC = %0.2f)' % (roc_auc3))\n",
    "plt.plot(fpr11, tpr11, label='Neural NetWork  (AUC = %0.2f)' % (roc_auc11))\n",
    "plt.plot(fpr8, tpr8, label='Ridge  (AUC = %0.2f)' % (roc_auc8))\n",
    "plt.plot(fpr9, tpr9, label='ElasticNet  (AUC = %0.2f)' % (roc_auc9))\n",
    "plt.plot(fpr6, tpr6, label='Logistic  (AUC = %0.2f)' % (roc_auc6))\n",
    "plt.plot(fpr7, tpr7, label='Lasso  (AUC = %0.2f)' % (roc_auc7))\n",
    "plt.plot(fpr10, tpr10, label='KNN  (AUC = %0.2f)' % (roc_auc10))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Random Classifier')   \n",
    "plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='green', label='Perfect Classifier')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic_importance[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb8fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIM = pd.DataFrame({'Features': train_X_ord.columns \n",
    "                   , 'CBC_Feature_importance':CBC_importance\n",
    "                   , 'GBM_Feature_importance':GBM_importance\n",
    "                   , 'XGB_Feature_importance':XGB_importance\n",
    "                   , 'RF_Feature_importance':RF_importance\n",
    "                   , 'LGB_Feature_importance':LGB_importance\n",
    "                   , 'AdB_Feature_importance':AdB_importance\n",
    "                   , 'Logistic_Feature_importance':Logistic_importance[0,:]\n",
    "                   , 'Ridge_Feature_importance':Ridge_importance[0,:]\n",
    "                   , 'Lasso_Feature_importance':Lasso_importance[0,:]\n",
    "                   , 'ElasticNet_Feature_importance':ElasticNet_importance[0,:]})\n",
    "FIM = FIM.sort_values(by=['AdB_Feature_importance'], ascending=False )\n",
    "display(FIM[['Features',\n",
    "             'AdB_Feature_importance',\n",
    "             'Logistic_Feature_importance',\n",
    "             'ElasticNet_Feature_importance',\n",
    "             'CBC_Feature_importance']].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.title('ADA Boost Feature Importance')\n",
    "FIM = FIM.sort_values(by=['AdB_Feature_importance'], ascending=False )\n",
    "sns.barplot(y='Features', x='AdB_Feature_importance', data=FIM.head(30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Cat Boost Feature Importance')\n",
    "FIM = FIM.sort_values(by=['CBC_Feature_importance'], ascending=False )\n",
    "sns.barplot(y='Features', x='CBC_Feature_importance', data=FIM.head(30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617ac0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "FIM = FIM.sort_values(by=['XGB_Feature_importance'], ascending=False )\n",
    "sns.barplot(y='Features', x='XGB_Feature_importance', data=FIM.head(30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2deec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Light Graident Boost Feature Importance')\n",
    "FIM = FIM.sort_values(by=['LGB_Feature_importance'], ascending=False )\n",
    "sns.barplot(y='Features', x='LGB_Feature_importance', data=FIM.head(30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb0ef4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def confusionmatrix(y_actual, y_pred, modellabel=''):\n",
    "    df=pd.DataFrame()\n",
    "    df['Actual']=y_actual\n",
    "    df['Predicted']=y_pred\n",
    "\n",
    "    confusion_matrix = pd.crosstab(df['Actual'], df['Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='g')\n",
    "\n",
    "    plt.title('Confusion Matrix for ' +modellabel+' Classifier')\n",
    "    plt.show()\n",
    "    \n",
    "confusionmatrix(test_y_ord, y_hat_AdB, modellabel='ADA boost')\n",
    "confusionmatrix(test_y_ord, y_hat_CBC, modellabel='Cat boost')\n",
    "confusionmatrix(test_y_ord, y_hat_XGB, modellabel='XGBoost')\n",
    "confusionmatrix(test_yd, yd_hat_Logistic, modellabel='Logistic Regression')\n",
    "confusionmatrix(test_y_ord, y_hat_GBM, modellabel='Light Gradient Boost')\n",
    "confusionmatrix(test_yd, yd_hat_ElasticNet, modellabel='ElasticNet Regression')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
